{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82fcbb86",
   "metadata": {
    "id": "view-in-github",
    "papermill": {
     "duration": 0.021014,
     "end_time": "2023-10-22T08:53:18.645310",
     "exception": false,
     "start_time": "2023-10-22T08:53:18.624296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/KayvanShah1/usc-csci-544-assignments-hw/blob/main/hw3/CSCI544_HW3_ver3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a065181f",
   "metadata": {
    "id": "5wMrhc8a_EjX",
    "papermill": {
     "duration": 0.020258,
     "end_time": "2023-10-22T08:53:18.687051",
     "exception": false,
     "start_time": "2023-10-22T08:53:18.666793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d4830",
   "metadata": {
    "id": "ChxDjnRnEJTa",
    "papermill": {
     "duration": 0.019988,
     "end_time": "2023-10-22T08:53:18.727162",
     "exception": false,
     "start_time": "2023-10-22T08:53:18.707174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0046e82d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T08:53:18.769790Z",
     "iopub.status.busy": "2023-10-22T08:53:18.769353Z",
     "iopub.status.idle": "2023-10-22T08:54:02.801854Z",
     "shell.execute_reply": "2023-10-22T08:54:02.800844Z"
    },
    "id": "s0OaIevOB4ey",
    "outputId": "29a244c0-58e9-4238-b1fe-1abd83777d62",
    "papermill": {
     "duration": 44.056934,
     "end_time": "2023-10-22T08:54:02.804506",
     "exception": false,
     "start_time": "2023-10-22T08:53:18.747572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\r\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\r\n",
      "Collecting textsearch>=0.0.21 (from contractions)\r\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\r\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\r\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\r\n",
      "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\r\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\r\n",
      "Collecting ipython-autotime\r\n",
      "  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\r\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.10/site-packages (from ipython-autotime) (8.14.0)\r\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython->ipython-autotime) (0.2.0)\r\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython->ipython-autotime) (5.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython->ipython-autotime) (0.18.2)\r\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython->ipython-autotime) (0.1.6)\r\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython->ipython-autotime) (0.7.5)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from ipython->ipython-autotime) (3.0.38)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython->ipython-autotime) (2.15.1)\r\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython->ipython-autotime) (0.6.2)\r\n",
      "Requirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.10/site-packages (from ipython->ipython-autotime) (5.9.0)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython->ipython-autotime) (4.8.0)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.3)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->ipython-autotime) (0.2.6)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->ipython-autotime) (1.2.0)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->ipython-autotime) (2.2.1)\r\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->ipython-autotime) (0.2.2)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython->ipython-autotime) (1.16.0)\r\n",
      "Installing collected packages: ipython-autotime\r\n",
      "Successfully installed ipython-autotime-0.3.1\r\n",
      "Collecting fastparquet\r\n",
      "  Downloading fastparquet-2023.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from fastparquet) (2.0.2)\r\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/conda/lib/python3.10/site-packages (from fastparquet) (1.23.5)\r\n",
      "Collecting cramjam>=2.3 (from fastparquet)\r\n",
      "  Downloading cramjam-2.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from fastparquet) (2023.9.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from fastparquet) (21.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2023.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->fastparquet) (3.0.9)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\r\n",
      "Installing collected packages: cramjam, fastparquet\r\n",
      "Successfully installed cramjam-2.7.0 fastparquet-2023.8.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install ipython-autotime\n",
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c6b1b4",
   "metadata": {
    "id": "B-sgJC0wEGqF",
    "papermill": {
     "duration": 0.023233,
     "end_time": "2023-10-22T08:54:02.853004",
     "exception": false,
     "start_time": "2023-10-22T08:54:02.829771",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f244506d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T08:54:02.902894Z",
     "iopub.status.busy": "2023-10-22T08:54:02.902470Z",
     "iopub.status.idle": "2023-10-22T08:54:20.319508Z",
     "shell.execute_reply": "2023-10-22T08:54:20.318323Z"
    },
    "id": "a7kG4_ic-tfe",
    "outputId": "190a1640-17f2-496f-dc57-0479c908511f",
    "papermill": {
     "duration": 17.444778,
     "end_time": "2023-10-22T08:54:20.321815",
     "exception": false,
     "start_time": "2023-10-22T08:54:02.877037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 356 µs (started: 2023-10-22 08:54:20 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import unicodedata\n",
    "import multiprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "import contractions\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import RandomSampler, BatchSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef93c4",
   "metadata": {
    "id": "MIBlsIzN4H9W",
    "papermill": {
     "duration": 0.023385,
     "end_time": "2023-10-22T08:54:20.369049",
     "exception": false,
     "start_time": "2023-10-22T08:54:20.345664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config\n",
    "\n",
    "Set up important configuration parameters and file paths for the project, making it easy to manage various settings and paths from one centralized location\n",
    "\n",
    "Place the `amazon_reviews_us_Office_Products_v1_00.tsv.gz` at the same level as noetbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef674733",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T08:54:20.418524Z",
     "iopub.status.busy": "2023-10-22T08:54:20.417902Z",
     "iopub.status.idle": "2023-10-22T08:54:20.430628Z",
     "shell.execute_reply": "2023-10-22T08:54:20.429528Z"
    },
    "id": "rxX0Bfswxc_4",
    "outputId": "cc27e43f-6758-440b-881c-8ce3b71df864",
    "papermill": {
     "duration": 0.040268,
     "end_time": "2023-10-22T08:54:20.432882",
     "exception": false,
     "start_time": "2023-10-22T08:54:20.392614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.75 ms (started: 2023-10-22 08:54:20 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# os.chdir(\"/content/drive/MyDrive/Colab Notebooks/CSCI544/HW3\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "CURRENT_DIR = os.getcwd()\n",
    "\n",
    "\n",
    "class DatasetConfig:\n",
    "    RANDOM_STATE = 34\n",
    "    TEST_SPLIT = 0.2\n",
    "    N_SAMPLES_EACH_CLASS = 50000\n",
    "    DATA_PATH = os.path.join(\n",
    "        CURRENT_DIR, \"amazon_reviews_us_Office_Products_v1_00.tsv.gz\"\n",
    "    )\n",
    "    PROCESSED_DATA_PATH = os.path.join(\n",
    "        CURRENT_DIR, \"amazon_review_processed_sentiment_analysis.parquet\"\n",
    "    )\n",
    "    PREPROCESSED_DATA_PATH = os.path.join(\n",
    "        CURRENT_DIR, \"amazon_review_preprocessed_sentiment_analysis.parquet\"\n",
    "    )\n",
    "    BUILD_NEW = True\n",
    "    if os.path.exists(PROCESSED_DATA_PATH) and os.path.exists(PREPROCESSED_DATA_PATH):\n",
    "        BUILD_NEW = False\n",
    "\n",
    "\n",
    "class Word2VecConfig:\n",
    "    PRETRAINED_MODEL = \"word2vec-google-news-300\"\n",
    "    PRETRAINED_DEFAULT_SAVE_PATH = os.path.join(\n",
    "        gensim.downloader.BASE_DIR, PRETRAINED_MODEL, f\"{PRETRAINED_MODEL}.gz\"\n",
    "    )\n",
    "    PRETRAINED_MODEL_SAVE_PATH = os.path.join(\n",
    "        CURRENT_DIR, PRETRAINED_MODEL, f\"{PRETRAINED_MODEL}.gz\"\n",
    "    )\n",
    "    WINDOW_SIZE = 13\n",
    "    MAX_LENGTH = 300\n",
    "    EMBEDDING_SIZE = 300\n",
    "    MIN_WORD_COUNT = 9\n",
    "    CUSTOM_MODEL_PATH = os.path.join(CURRENT_DIR, \"word2vec-custom.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6a9e0d",
   "metadata": {
    "papermill": {
     "duration": 0.022669,
     "end_time": "2023-10-22T08:54:20.479406",
     "exception": false,
     "start_time": "2023-10-22T08:54:20.456737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Change the IOPub rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59bea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!timeout 10s jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10 --allow-root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c1811a",
   "metadata": {
    "id": "RG9DJh9dWaTK",
    "papermill": {
     "duration": 0.024614,
     "end_time": "2023-10-22T08:54:32.259948",
     "exception": false,
     "start_time": "2023-10-22T08:54:32.235334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99513b4",
   "metadata": {
    "id": "5e3w--t7WkKq",
    "papermill": {
     "duration": 0.023958,
     "end_time": "2023-10-22T08:54:32.308845",
     "exception": false,
     "start_time": "2023-10-22T08:54:32.284887",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Download & Save Pretrained model\n",
    "\n",
    "- Run the `api.load()` once and copied the model from temporary path to local drive for fast loading of model in memory.\n",
    "\n",
    "### References:\n",
    "1. [Faster way to load word2vec model](https://github.com/RaRe-Technologies/gensim/issues/2642)\n",
    "2. [Tutorial](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e5b6587",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T08:54:32.357847Z",
     "iopub.status.busy": "2023-10-22T08:54:32.357416Z",
     "shell.execute_reply": "2023-10-22T08:57:54.047533Z"
    },
    "id": "BbfiVi5wWesO",
    "outputId": "60f3aeb9-629e-4b77-feab-13e4d51f2bd8",
    "papermill": {
     "duration": 205.719052,
     "end_time": "2023-10-22T08:57:58.051484",
     "exception": false,
     "start_time": "2023-10-22T08:54:32.332432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================================--------] 85.0% 1413.9/1662.8MB downloaded"
     ]
    }
   ],
   "source": [
    "def load_pretrained_model():\n",
    "    if not os.path.exists(Word2VecConfig.PRETRAINED_MODEL_SAVE_PATH):\n",
    "        # Create a directory if it doesn't exist\n",
    "        os.makedirs(Word2VecConfig.PRETRAINED_MODEL, exist_ok=True)\n",
    "        # Download the embeddings\n",
    "        api.load(Word2VecConfig.PRETRAINED_MODEL, return_path=True)\n",
    "        # Copy & save the embeddings file\n",
    "        shutil.copyfile(\n",
    "            Word2VecConfig.PRETRAINED_DEFAULT_SAVE_PATH, Word2VecConfig.PRETRAINED_MODEL_SAVE_PATH\n",
    "        )\n",
    "    \n",
    "    # Load the word2vec embeddings\n",
    "    pretrained_model = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format(\n",
    "        Word2VecConfig.PRETRAINED_MODEL_SAVE_PATH, binary=True\n",
    "    )\n",
    "    return pretrained_model\n",
    "\n",
    "\n",
    "# Load the pretrained model\n",
    "pretrained_model = load_pretrained_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09794ac",
   "metadata": {
    "id": "BGk7MFgi5fQM",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Accelarator Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc42709",
   "metadata": {
    "id": "k3HFKs015mI3",
    "outputId": "9c3ef98a-47c1-4003-cb6b-4e6e5b1f3009",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        # Check if GPU is available\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        # Use CPU if no GPU or TPU is available\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bed98f",
   "metadata": {
    "id": "Xl28Fzew1Ua9",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Download Data\n",
    "\n",
    "Checks if a file specified by `DatasetConfig.DATA_PATH` exists. If not, it downloads the file from a given URL and saves it with the same name. If the file already exists, it prints a message indicating so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083ebc10",
   "metadata": {
    "id": "nkn2x3me1YZ6",
    "outputId": "8c6f350b-c866-4af5-8bad-b083f69b4522",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(DatasetConfig.DATA_PATH):\n",
    "    url = (\n",
    "        \"https://web.archive.org/web/20201127142707if_/https://s3.amazonaws.com/amazon-reviews-pds\"\n",
    "        \"/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz\"\n",
    "    )\n",
    "    file_name = DatasetConfig.DATA_PATH\n",
    "\n",
    "    # Stream and download heavy file in chunks\n",
    "    with requests.get(url, stream=True) as response:\n",
    "        with open(file_name, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "\n",
    "    print(f\"Downloaded file '{os.path.relpath(file_name)}' successfully.\")\n",
    "else:\n",
    "    print(f\"File '{DatasetConfig.DATA_PATH}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf3fb34",
   "metadata": {
    "id": "1lUVynXQ38dL",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "This code provides a pipeline for processing and preparing a dataset for sentiment analysis:\n",
    "\n",
    "1. `LoadData` class loads a dataset from a specified path, keeping only relevant columns.\n",
    "\n",
    "2. `ProcessData` class performs the following tasks:\n",
    "   - Converts star ratings to numeric values.\n",
    "   - Classifies sentiments based on star ratings (1 for negative, 2 for positive).\n",
    "   - Balances the dataset by sampling an equal number of samples for both sentiments.\n",
    "\n",
    "3. `CleanText` class defines various text cleaning operations:\n",
    "   - Removing non-ASCII characters.\n",
    "   - Expanding contractions.\n",
    "   - Removing email addresses, URLs, and HTML tags.\n",
    "   - Lowercasing and stripping spaces.\n",
    "\n",
    "4. `clean_and_process_data` function executes the entire data processing pipeline:\n",
    "   - Loads the data.\n",
    "   - Applies basic processing.\n",
    "   - Balances the dataset.\n",
    "   - Cleans the text.\n",
    "   - Tokenizes the reviews.\n",
    "\n",
    "5. `preprocess_review_body` function generates word embeddings for each word in a review using a pre-trained Word2Vec model.\n",
    "\n",
    "6. `get_reviews_dataset` function handles the entire data preprocessing and embedding generation process. It checks if the preprocessed data already exists, and if not, it performs the data preprocessing and saves the preprocessed data in Parquet format.\n",
    "\n",
    "Overall, this pipeline ensures that the dataset is properly loaded, cleaned, processed, balanced, and transformed into embeddings suitable for sentiment analysis.\n",
    "\n",
    "> Note:\n",
    "> - Parquet format is efficient for storage.\n",
    "> - Storing data to avoid running the pipeline and embedding generation process all over again.\n",
    "> - Provides a ready-to-use dataset for sentiment analysis tasks, allowing for quicker experimentation and model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b2d7a9",
   "metadata": {
    "id": "sqe3kdXz4f2K",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Read and Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912b3407",
   "metadata": {
    "id": "KTuRzMHtIlx0",
    "outputId": "98956e87-b833-461b-ac12-a463017bccd2",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LoadData:\n",
    "    @staticmethod\n",
    "    def load_data(path):\n",
    "        df = pd.read_csv(\n",
    "            path,\n",
    "            sep=\"\\t\",\n",
    "            usecols=[\"review_headline\", \"review_body\", \"star_rating\"],\n",
    "            on_bad_lines=\"skip\",\n",
    "            memory_map=True,\n",
    "        )\n",
    "        return df\n",
    "\n",
    "\n",
    "class ProcessData:\n",
    "    @staticmethod\n",
    "    def filter_columns(df):\n",
    "        return df.loc[:, [\"review_body\", \"star_rating\"]]\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_star_rating(df):\n",
    "        df[\"star_rating\"] = pd.to_numeric(df[\"star_rating\"], errors=\"coerce\")\n",
    "        df.dropna(subset=[\"star_rating\"], inplace=True)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def classify_sentiment(df):\n",
    "        df[\"sentiment\"] = df[\"star_rating\"].apply(lambda x: 1 if x <= 3 else 2)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_data(df, n_samples, random_state):\n",
    "        sampled_df = pd.concat(\n",
    "            [\n",
    "                df.query(\"sentiment==1\").sample(n=n_samples, random_state=random_state),\n",
    "                df.query(\"sentiment==2\").sample(n=n_samples, random_state=random_state),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        ).sample(frac=1, random_state=random_state, ignore_index=True)\n",
    "\n",
    "        sampled_df.drop(columns=[\"star_rating\"], inplace=True)\n",
    "        return sampled_df\n",
    "\n",
    "\n",
    "class CleanText:\n",
    "    @staticmethod\n",
    "    def unicode_to_ascii(s):\n",
    "        return \"\".join(\n",
    "            c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def expand_contractions(text):\n",
    "        \"\"\"Expand contraction for eg., wouldn't => would not\"\"\"\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_email_addresses(text):\n",
    "        return re.sub(r\"[a-zA-Z0-9_\\-\\.]+@[a-zA-Z0-9_\\-\\.]+\\.[a-zA-Z]{2,5}\", \"\", text)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_urls(text):\n",
    "        return re.sub(r\"\\bhttps?:\\/\\/\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_html_tags(text):\n",
    "        return re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        text = text.lower().strip()\n",
    "        text = CleanText.unicode_to_ascii(text)\n",
    "        # text = CleanText.remove_email_addresses(text)\n",
    "        # text = CleanText.remove_urls(text)\n",
    "        text = CleanText.remove_html_tags(text)\n",
    "        text = CleanText.expand_contractions(text)\n",
    "\n",
    "        # creating a space between a word and the punctuation following it\n",
    "        # text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
    "        # text = re.sub(r'[\" \"]+', \" \", text)\n",
    "\n",
    "        # removes all non-alphabetical characters\n",
    "        # text = re.sub(r\"[^a-zA-Z\\s]+\", \"\", text)\n",
    "\n",
    "        # remove extra spaces\n",
    "        # text = re.sub(\" +\", \" \", text)\n",
    "        return text\n",
    "\n",
    "\n",
    "def clean_and_process_data(path):\n",
    "    df = LoadData.load_data(path)\n",
    "\n",
    "    # Basic processing\n",
    "    df_filtered = ProcessData.filter_columns(df)\n",
    "    df_filtered = ProcessData.convert_star_rating(df_filtered)\n",
    "    df_filtered = ProcessData.classify_sentiment(df_filtered)\n",
    "\n",
    "    balanced_df = ProcessData.sample_data(\n",
    "        df_filtered, DatasetConfig.N_SAMPLES_EACH_CLASS, DatasetConfig.RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    # Clean data\n",
    "    balanced_df.dropna(inplace=True)\n",
    "    balanced_df[\"review_body\"] = balanced_df[\"review_body\"].astype(str)\n",
    "    balanced_df[\"review_body\"] = balanced_df[\"review_body\"].apply(CleanText.clean_text)\n",
    "    # Drop reviews that are empty\n",
    "    balanced_df = balanced_df.loc[balanced_df[\"review_body\"].str.strip() != \"\"]\n",
    "\n",
    "    # Tokenize Reviews\n",
    "    balanced_df[\"review_body\"] = balanced_df[\"review_body\"].apply(word_tokenize)\n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "def preprocess_review_body(text, word2vec_model, topn=None):\n",
    "    embeddings = [word2vec_model[word] for word in text if word in word2vec_model]\n",
    "\n",
    "    if topn is not None:\n",
    "        embeddings = np.concatenate(embeddings[:topn], axis=0)\n",
    "    else:\n",
    "        embeddings = np.mean(embeddings, axis=0)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_reviews_dataset(new=False):\n",
    "    if new or not os.path.exists(DatasetConfig.DATA_PATH):\n",
    "        balanced_df = clean_and_process_data(DatasetConfig.DATA_PATH)\n",
    "        balanced_df.to_parquet(DatasetConfig.PROCESSED_DATA_PATH, index=False)\n",
    "\n",
    "        # Preprocess data and generate word2vec embeddings Avg and top 10\n",
    "        balanced_df[\"embeddings\"] = balanced_df[\"review_body\"].apply(\n",
    "            lambda text: preprocess_review_body(text, pretrained_model, topn=None)\n",
    "        )\n",
    "        # Drop rows with NaN embeddings\n",
    "        balanced_df.dropna(subset=[\"embeddings\"], inplace=True)\n",
    "\n",
    "        balanced_df[\"embeddings_top_10\"] = balanced_df[\"review_body\"].apply(\n",
    "            lambda text: preprocess_review_body(text, pretrained_model, topn=10)\n",
    "        )\n",
    "\n",
    "        balanced_df.to_parquet(DatasetConfig.PREPROCESSED_DATA_PATH, index=False)\n",
    "    else:\n",
    "        balanced_df = pd.read_parquet(\n",
    "            DatasetConfig.PREPROCESSED_DATA_PATH,\n",
    "            # engine=\"fastparquet\"\n",
    "        )\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857480a",
   "metadata": {
    "id": "CwsDbNoi_1Oc",
    "outputId": "fcd4981f-140a-4f36-c511-44ff1366f888",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "balanced_df = get_reviews_dataset(\n",
    "    new=DatasetConfig.BUILD_NEW\n",
    ")\n",
    "print(\"Total Records:\", balanced_df.shape)\n",
    "balanced_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2934d4",
   "metadata": {
    "id": "bttIYVk-mxfl",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Review Body stats\n",
    "Mean number of words = 66\n",
    "\n",
    "Median number of words = 37\n",
    "\n",
    "Limiting sequence length for RNN based embeddings = 45\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba7efa",
   "metadata": {
    "id": "S2ZlVh0plY2q",
    "outputId": "8dcf0d40-9c39-458d-ee73-34ef7f1ef912",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "balanced_df[\"review_body\"].apply(len).describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0203a178",
   "metadata": {
    "id": "REP4Ohx2MfFc",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Train, Valid and Test Spilts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb37427d",
   "metadata": {
    "id": "eppCI8v4IO8p",
    "outputId": "0ffa5aff-d104-4c1c-893c-3c532e019585",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create train and temp sets (80% train, 20% valid + test)\n",
    "train_df, valid_df = train_test_split(\n",
    "    balanced_df,\n",
    "    test_size=0.20,\n",
    "    random_state=DatasetConfig.RANDOM_STATE,\n",
    "    stratify=balanced_df[\"sentiment\"]\n",
    ")\n",
    "\n",
    "# Create valid and test sets (15% valid, 5% test)\n",
    "valid_df, test_df = train_test_split(\n",
    "    valid_df,\n",
    "    test_size=0.25,  # 25% of 20% is 5%\n",
    "    random_state=DatasetConfig.RANDOM_STATE,\n",
    "    stratify=valid_df[\"sentiment\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77beafdc",
   "metadata": {
    "id": "01xZp5gDGS3Y",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Word Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94b9d7f",
   "metadata": {
    "id": "cz6qCPYqGSVK",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Semantic similarity examples with pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10cb59d",
   "metadata": {
    "id": "CUXIBxdPyOsP",
    "outputId": "24df754d-6ec2-4ef8-8e52-4b1b1ebe8830",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example 1: King - Man + Woman = Queen\n",
    "result = pretrained_model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(f\"Semantic Similarity: {result[0][0]}\")\n",
    "\n",
    "# Example 2: excellent ~ outstanding\n",
    "result = pretrained_model.similarity('excellent', 'outstanding')\n",
    "print(f\"Semantic Similarity: {result}\")\n",
    "\n",
    "# Example 3: Paris - France + Italy = Milan\n",
    "result = pretrained_model.most_similar(positive=['Italy', 'Paris'], negative=['France'])\n",
    "print(f\"Semantic Similarity: {result[0][0]}\")\n",
    "\n",
    "# Example 4: Car - Wheel + Boat = Yacht\n",
    "result = pretrained_model.most_similar(positive=['Boat', 'Car'], negative=['Wheel'])\n",
    "print(f\"Semantic Similarity: {result[0][0]}\")\n",
    "\n",
    "# Example 5: Delicious ~ Tasty\n",
    "result = pretrained_model.similarity('Delicious', 'Tasty')\n",
    "print(f\"Semantic Similarity: {result}\")\n",
    "\n",
    "# Example 6: Computer ~ Plant\n",
    "result = pretrained_model.similarity('Computer', 'Plant')\n",
    "print(f\"Semantic Similarity: {result}\")\n",
    "\n",
    "# Example 7: Cat ~ Dog\n",
    "result = pretrained_model.similarity('Cat', 'Dog')\n",
    "print(f\"Semantic Similarity: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4ef0b3",
   "metadata": {
    "id": "fLKdQgxlZH5J",
    "outputId": "e41f4e35-55b7-455c-c14c-be207d2396d9",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca37f5",
   "metadata": {
    "id": "Gbe7-WOzOitC",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Custom Word2Vec Embeddings Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c714262",
   "metadata": {
    "id": "gHA1YeIJPjIz",
    "outputId": "6aecf6b1-864e-4591-b343-d5da7a71361a",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences=train_df[\"review_body\"].tolist()\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model_custom = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=Word2VecConfig.MAX_LENGTH,\n",
    "    window=Word2VecConfig.WINDOW_SIZE,\n",
    "    min_count=Word2VecConfig.MIN_WORD_COUNT,\n",
    "    workers=multiprocessing.cpu_count()\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "w2v_model_custom.save(Word2VecConfig.CUSTOM_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a08c7a6",
   "metadata": {
    "id": "dnPj5ypZWiw-",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Test Custom Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3cbbde",
   "metadata": {
    "id": "-IFM8OadWmtl",
    "outputId": "8716c073-2bc5-4b90-8fdd-31beda287f66",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the custom model\n",
    "w2v_model_custom = Word2Vec.load(Word2VecConfig.CUSTOM_MODEL_PATH)\n",
    "\n",
    "# Example 1: King - Man + Woman = Queen\n",
    "res = w2v_model_custom.wv.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(f\"Semantic Similarity (Custom Model): {res[0]}\")\n",
    "\n",
    "# Example 2: excellent ~ outstanding\n",
    "res = w2v_model_custom.wv.similarity('excellent', 'outstanding')\n",
    "print(f\"Semantic Similarity (Custom Model): {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d200e64",
   "metadata": {
    "id": "YZxUyismSi3e",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "**What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better?**\n",
    "\n",
    "1. **Custom-trained Word2Vec Model:**\n",
    "   - **Strengths:**\n",
    "        - Captures domain-specific relationships and nuances as it trained on very specific dataset.\n",
    "   - **Weaknesses:**\n",
    "        - It may not perform as well on tasks outside of its training domain.\n",
    "        - The quality of embeddings heavily depends on the dataset used for training.\n",
    "        - For example, if the dataset is small or not representative of the overall language, the embeddings may be less reliable.\n",
    "\n",
    "2. **Pretrained \"word2vec-google-news-300\" Model:**\n",
    "   - **Strengths:**\n",
    "        - This model has been pretrained on a massive corpus of text from various domains, making it highly versatile and capable of capturing a wide range of semantic relationships.\n",
    "        - It can generalize well to different tasks and domains.\n",
    "   - **Weaknesses:**\n",
    "        - While it provides strong generalization, it may not capture domain-specific relationships as effectively as a model trained on domain-specific data.\n",
    "\n",
    "- The semantic similarity score is higher for the pretrained model compared to the custom model. This indicates that the pretrained model is better at encoding semantic similarities between words.\n",
    "- The custom Word2Vec model, which was trained on the provided dataset, may not have had access to as diverse and extensive a corpus as the pretrained model. This can lead to limitations in its ability to generalize and capture nuanced semantic relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e14568",
   "metadata": {
    "id": "cb0TSKVPbPcf",
    "outputId": "aa2b50d1-0b2f-47a2-f8f2-463142fccfd4",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del w2v_model_custom, res, sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17b5531",
   "metadata": {
    "id": "1YrcemHfPoZQ",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Simple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0823e023",
   "metadata": {
    "id": "_HgfCvdfcbHQ",
    "outputId": "be689357-6fe4-42db-f2a1-7536a59ba986",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    precision = precision_score(y_test, y_pred, average=\"binary\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"binary\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"binary\")\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return precision, recall, f1, accuracy\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(model_class, X_train, y_train, X_test, y_test, **model_params):\n",
    "    # Initialize model\n",
    "    model = model_class(**model_params)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate model\n",
    "    precision, recall, f1, accuracy = evaluate_model(model, X_test, y_test)\n",
    "    return model, precision, recall, f1, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d249b0f",
   "metadata": {
    "id": "OprU4SiQsrEx",
    "outputId": "baba596f-2d47-451a-b602-dd7aa41276b4",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = np.vstack(train_df[\"embeddings\"])\n",
    "y_train = train_df[\"sentiment\"]\n",
    "X_test = np.vstack(test_df[\"embeddings\"])\n",
    "y_test = test_df[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df01746b",
   "metadata": {
    "id": "b2lH8Dv4bxIM",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SVM\n",
    "\n",
    "| Params | Precision | Recall | F1 | Accuracy |  Features Used |\n",
    "|--------|-----------| -------| ---| --- | --- |\n",
    "| LinearSVC(C=0.1, max_iter=10000) |  0.7997 | 0.8671 | 0.8320 | 0.8321 |  Word2Vec      |\n",
    "| LinearSVC(max_iter=10000) | 0.8045 | 0.8623 | 0.8324 | 0.8262 |  Word2Vec      |\n",
    "| LinearSVC(C=0.01, max_iter=15000) | 0.7836 | 0.8835 | 0.8305 | 0.8281 |  Word2Vec      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e569b5",
   "metadata": {
    "id": "OPNG5jhmS4JP",
    "outputId": "e329c186-17de-4456-f7af-fae52a02245a",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train and evaluate LinearSVC model\n",
    "(\n",
    "    _,\n",
    "    precision_svc,\n",
    "    recall_svc,\n",
    "    f1_svc,\n",
    "    acc_svc\n",
    ") = train_and_evaluate_model(\n",
    "    LinearSVC,\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    max_iter=10000,\n",
    "    # C=0.1\n",
    ")\n",
    "\n",
    "print(f'Precision Recall F1 Accuracy (LinearSVC): {precision_svc:.4f} {recall_svc:.4f} {f1_svc:.4f} {acc_svc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1e187d",
   "metadata": {
    "id": "bU1RkC-fb0Cr",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Perceptron\n",
    "\n",
    "| Params | Precision | Recall | F1 | Accuracy | Features Used |\n",
    "|--------|-----------| ------ | -- | ---- | ------- |\n",
    "| Perceptron(eta0=0.01, max_iter=5000, penalty='elasticnet', warm_start=True) | 0.7693 | 0.8778 | 0.8200 | 0.8071 |  Word2Vec      |\n",
    "| Perceptron(max_iter=5000) | 0.7786 | 0.8613 | 0.8179 | 0.8110 |  Word2Vec      |\n",
    "| Perceptron() | 0.7786 | 0.8613 | 0.8179 | 0.8110 |  Word2Vec      |\n",
    "| Perceptron(eta0=0.1, max_iter=5000, penalty='elasticnet', warm_start=True) | 0.5977 | 0.9844 | 0.7438 | 0.6655 | Word2Vec      |\n",
    "| Perceptron(eta0=0.001, max_iter=10000, penalty='l2') | 0.7367 | 0.9114 | 0.8148 | 0.7849 | Word2Vec      |\n",
    "| Perceptron(eta0=0.01, max_iter=10000, penalty='l2', warm_start=True) | 0.7653 | 0.8789 | 0.8181 | 0.8002 | Word2Vec      |\n",
    "| Perceptron(eta0=0.01, penalty='l1', warm_start=True) | 0.6133 | 0.9813 | 0.7548 | 0.6832 | Word2Vec      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9685d530",
   "metadata": {
    "id": "Zv524ryZcHoK",
    "outputId": "9f840ae0-b478-4a36-be3c-9f7686c8045d",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train and evaluate Perceptron model using BoW features\n",
    "(\n",
    "    _,\n",
    "    precision_perceptron,\n",
    "    recall_perceptron,\n",
    "    f1_perceptron,\n",
    "    acc_perceptron\n",
    ") = train_and_evaluate_model(\n",
    "    Perceptron,\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    max_iter=5000,\n",
    "    eta0=0.01,\n",
    "    warm_start=True,\n",
    "    penalty=\"elasticnet\"\n",
    ")\n",
    "\n",
    "print(f'Precision Recall F1 (Perceptron): {precision_perceptron:.4f} {recall_perceptron:.4f} {f1_perceptron:.4f} {acc_perceptron:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b5600",
   "metadata": {
    "id": "UbFANC3u6spZ",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## With TFIDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f088cad",
   "metadata": {
    "cellView": "form",
    "id": "REJGdZsW6yvI",
    "outputId": "957cf689-ebc1-4cea-b570-dde0bbc35069",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Homework 1 Script Edited\n",
    "\n",
    "%%writefile HW1-CSCI544-wo-neg-sw.py\n",
    "# Python Version: 3.10.12\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "\n",
    "import contractions\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "class Config:\n",
    "    RANDOM_STATE = 56\n",
    "    DATA_PATH = \"amazon_reviews_us_Office_Products_v1_00.tsv.gz\"\n",
    "    TEST_SPLIT = 0.2\n",
    "    N_SAMPLES_EACH_CLASS = 50000\n",
    "    NUM_TFIDF_FEATURES = 5000\n",
    "    NUM_BOW_FEATURES = 5000\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    @staticmethod\n",
    "    def load_data(path):\n",
    "        df = pd.read_csv(\n",
    "            path,\n",
    "            sep=\"\\t\",\n",
    "            usecols=[\"review_headline\", \"review_body\", \"star_rating\"],\n",
    "            on_bad_lines=\"skip\",\n",
    "            memory_map=True,\n",
    "        )\n",
    "        return df\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    @staticmethod\n",
    "    def filter_columns(df):\n",
    "        return df.loc[:, [\"review_body\", \"star_rating\"]]\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_star_rating(df):\n",
    "        df[\"star_rating\"] = pd.to_numeric(df[\"star_rating\"], errors=\"coerce\")\n",
    "        df.dropna(subset=[\"star_rating\"], inplace=True)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def classify_sentiment(df):\n",
    "        df[\"sentiment\"] = df[\"star_rating\"].apply(lambda x: 1 if x <= 3 else 2)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_data(df, n_samples, random_state):\n",
    "        sampled_df = pd.concat(\n",
    "            [\n",
    "                df.query(\"sentiment==1\").sample(n=n_samples, random_state=random_state),\n",
    "                df.query(\"sentiment==2\").sample(n=n_samples, random_state=random_state),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        ).sample(frac=1, random_state=random_state)\n",
    "\n",
    "        sampled_df.drop(columns=[\"star_rating\"], inplace=True)\n",
    "        return sampled_df\n",
    "\n",
    "\n",
    "class TextCleaner:\n",
    "    @staticmethod\n",
    "    def unicode_to_ascii(s):\n",
    "        return \"\".join(\n",
    "            c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def expand_contractions(text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_email_addresses(text):\n",
    "        return re.sub(r\"[a-zA-Z0-9_\\-\\.]+@[a-zA-Z0-9_\\-\\.]+\\.[a-zA-Z]{2,5}\", \" \", text)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_urls(text):\n",
    "        return re.sub(r\"\\bhttps?:\\/\\/\\S+|www\\.\\S+\", \" \", text)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_html_tags(text):\n",
    "        return re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        text = TextCleaner.unicode_to_ascii(text.lower().strip())\n",
    "        # replacing email addresses with empty string\n",
    "        text = TextCleaner.remove_email_addresses(text)\n",
    "        # replacing urls with empty string\n",
    "        text = TextCleaner.remove_urls(text)\n",
    "        # Remove HTML tags\n",
    "        text = TextCleaner.remove_html_tags(text)\n",
    "        # Expand contraction for eg., wouldn't => would not\n",
    "        text = TextCleaner.expand_contractions(text)\n",
    "        # creating a space between a word and the punctuation following it\n",
    "        text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
    "        text = re.sub(r'[\" \"]+', \" \", text)\n",
    "        # removes all non-alphabetical characters\n",
    "        text = re.sub(r\"[^a-zA-Z\\s]+\", \"\", text)\n",
    "        # remove extra spaces\n",
    "        text = re.sub(\" +\", \" \", text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_stopwords_pattern():\n",
    "        # Stopword list\n",
    "        og_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "        # Define a list of negative words to remove\n",
    "        neg_words = [\"no\", \"not\", \"nor\", \"neither\", \"none\", \"never\", \"nobody\", \"nowhere\"]\n",
    "        custom_stopwords = [word for word in og_stopwords if word not in neg_words]\n",
    "        pattern = re.compile(r\"\\b(\" + r\"|\".join(custom_stopwords) + r\")\\b\\s*\")\n",
    "        return pattern\n",
    "\n",
    "    @staticmethod\n",
    "    def pos_tagger(tag):\n",
    "        if tag.startswith(\"J\"):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith(\"V\"):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith(\"N\"):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith(\"R\"):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def lemmatize_text_using_pos_tags(text):\n",
    "        words = nltk.pos_tag(word_tokenize(text))\n",
    "        words = map(lambda x: (x[0], TextPreprocessor.pos_tagger(x[1])), words)\n",
    "        lemmatized_words = [\n",
    "            TextPreprocessor.lemmatizer.lemmatize(word, tag) if tag else word for word, tag in words\n",
    "        ]\n",
    "        return \" \".join(lemmatized_words)\n",
    "\n",
    "    @staticmethod\n",
    "    def lemmatize_text(text):\n",
    "        words = word_tokenize(text)\n",
    "        lemmatized_words = [TextPreprocessor.lemmatizer.lemmatize(word) for word in words]\n",
    "        return \" \".join(lemmatized_words)\n",
    "\n",
    "    pattern = get_stopwords_pattern()\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_text(text):\n",
    "        # replacing all the stopwords\n",
    "        text = TextPreprocessor.pattern.sub(\"\", text)\n",
    "        text = TextPreprocessor.lemmatize_text(text)\n",
    "        return text\n",
    "\n",
    "\n",
    "clean_text_vect = np.vectorize(TextCleaner.clean_text)\n",
    "preprocess_text_vect = np.vectorize(TextPreprocessor.preprocess_text)\n",
    "\n",
    "\n",
    "def clean_and_process_data(path):\n",
    "    df = DataLoader.load_data(path)\n",
    "    df_filtered = DataProcessor.filter_columns(df)\n",
    "    df_filtered = DataProcessor.convert_star_rating(df_filtered)\n",
    "    df_filtered = DataProcessor.classify_sentiment(df_filtered)\n",
    "\n",
    "    balanced_df = DataProcessor.sample_data(\n",
    "        df_filtered, Config.N_SAMPLES_EACH_CLASS, Config.RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    balanced_df[\"review_body\"] = balanced_df[\"review_body\"].astype(str)\n",
    "\n",
    "    # Clean data\n",
    "    # avg_len_before_clean = balanced_df[\"review_body\"].apply(len).mean()\n",
    "    balanced_df[\"review_body\"] = balanced_df[\"review_body\"].apply(clean_text_vect)\n",
    "    # Drop reviews that are empty\n",
    "    balanced_df = balanced_df.loc[balanced_df[\"review_body\"].str.strip() != \"\"]\n",
    "    # avg_len_after_clean = balanced_df[\"review_body\"].apply(len).mean()\n",
    "\n",
    "    # Preprocess data\n",
    "    # avg_len_before_preprocess = avg_len_after_clean\n",
    "    balanced_df[\"review_body\"] = balanced_df[\"review_body\"].apply(preprocess_text_vect)\n",
    "    # avg_len_after_preprocess = balanced_df[\"review_body\"].apply(len).mean()\n",
    "\n",
    "    # Print Results\n",
    "    # print(f\"{avg_len_before_clean:.2f}, {avg_len_after_clean:.2f}\")\n",
    "    # print(f\"{avg_len_before_preprocess:.2f}, {avg_len_after_preprocess:.2f}\")\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    precision = precision_score(y_test, y_pred, average=\"binary\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"binary\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"binary\")\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return precision, recall, f1, accuracy\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(model_class, X_train, y_train, X_test, y_test, **model_params):\n",
    "    # Initialize model\n",
    "    model = model_class(**model_params)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate model\n",
    "    precision, recall, f1, accuracy = evaluate_model(model, X_test, y_test)\n",
    "    return model, precision, recall, f1, accuracy\n",
    "\n",
    "\n",
    "def main():\n",
    "    balanced_df = clean_and_process_data(Config.DATA_PATH)\n",
    "\n",
    "    # Splitting the reviews dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        balanced_df[\"review_body\"],\n",
    "        balanced_df[\"sentiment\"],\n",
    "        test_size=Config.TEST_SPLIT,\n",
    "        random_state=Config.RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    # Feature Extraction\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=Config.NUM_TFIDF_FEATURES)\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    # Train and evaluate Perceptron model using TF-IDF features\n",
    "    (\n",
    "        _,\n",
    "        precision_perceptron_tfidf,\n",
    "        recall_perceptron_tfidf,\n",
    "        f1_perceptron_tfidf,\n",
    "        acc_perceptron_tfidf\n",
    "    ) = train_and_evaluate_model(\n",
    "        Perceptron, X_train_tfidf, y_train, X_test_tfidf, y_test, max_iter=4000\n",
    "    )\n",
    "\n",
    "    # Train and evaluate SVM model using TF-IDF features\n",
    "    (\n",
    "        _,\n",
    "        precision_svm_tfidf,\n",
    "        recall_svm_tfidf,\n",
    "        f1_svm_tfidf,\n",
    "        acc_svm_tfidf\n",
    "    ) = train_and_evaluate_model(\n",
    "        LinearSVC, X_train_tfidf, y_train, X_test_tfidf, y_test, max_iter=2500\n",
    "    )\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Precision Recall F1-Score Accuracy\")\n",
    "    print(\"Perceptron\")\n",
    "    print(\n",
    "        f\"{precision_perceptron_tfidf:.4f} {recall_perceptron_tfidf:.4f} {f1_perceptron_tfidf:.4f} {acc_perceptron_tfidf:.4f}\"\n",
    "    )\n",
    "\n",
    "    print(\"SVM: LinearSVC\")\n",
    "    print(f\"{precision_svm_tfidf:.4f} {recall_svm_tfidf:.4f} {f1_svm_tfidf:.4f} {acc_svm_tfidf:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89251ed1",
   "metadata": {
    "id": "3zFUOjcG8MwH",
    "outputId": "7e9a1270-d94d-4851-c1db-5bcbf99783b0",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python HW1-CSCI544-wo-neg-sw.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf67d4d",
   "metadata": {
    "id": "1koZ_3RVb3ua",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "### Best Accuracies\n",
    "\n",
    "| Model | Accuracy | Features Used |\n",
    "|--------|-----------| ------- |\n",
    "| Perceptron | 0.8110 | Word2Vec |\n",
    "| LinearSVC | 0.8321 | Word2Vec |\n",
    "| Perceptron | 0.7998 | TF-IDF |\n",
    "| LinearSVC | 0.8581 | TF-IDF |\n",
    "\n",
    "1. LinearSVC outperforms Perceptron for both feature types (Word2Vec and TF-IDF).\n",
    "    - LinearSVC is better suited for this classification task compared to Perceptron.\n",
    "\n",
    "2. When using Word2Vec features, both Perceptron and LinearSVC achieve lower accuracy compared to when using TF-IDF features.\n",
    "    - Word2Vec embeddings might not be as effective for this specific sentiment classification task as compared to TF-IDF vectors.\n",
    "\n",
    "3. The LinearSVC model performs particularly well with TF-IDF features, achieving an accuracy of 85.81%.\n",
    "    - TF-IDF vectors are highly effective in capturing important information for sentiment classification in this dataset.\n",
    "\n",
    "Overall, based on the provided performance metrics, it seems that TF-IDF features are more effective for this sentiment classification task compared to the Word2Vec embeddings. However, it's important to note that the effectiveness of features can vary depending on the specific dataset and task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4335745",
   "metadata": {
    "id": "SxsWcM-Gb72I",
    "outputId": "4e3bf509-ec09-426c-d1dc-dc1921992052",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del balanced_df\n",
    "del X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f884e",
   "metadata": {
    "id": "CG7tGa8UxmZM",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Pytorch Dataset\n",
    "\n",
    "- Custom pytorch dataset for on-the-fly processing an d efficient resource utilization\n",
    "- Each sample in this dataset includes embeddings and their corresponding target label. The label is adjusted by subtracting 1 from the label value in the DataFrame\n",
    "- Using `DataLoader`'s\n",
    "    - Used to load and manage batches of data during the training process.\n",
    "    - Handle tasks like shuffling, batching, and parallel data loading, making it easier to feed data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1685865c",
   "metadata": {
    "id": "flCZXZDWSmw8",
    "outputId": "7d4e95f1-0bd6-4e73-8828-c87549083a05",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AmazonReviewsSentimentDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df, embeddings_col_name: str, label_col_name: str,\n",
    "        max_length=None, flatten: bool=True,\n",
    "        embedding_size: int=None, num_seq: int=None\n",
    "    ):\n",
    "        \"\"\"Dataset class for Amazon Reviews Sentiment Analysis.\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): The input DataFrame containing the data.\n",
    "            embeddings_col_name (str): The column name for the embeddings.\n",
    "            label_col_name (str): The column name for the labels.\n",
    "            max_length (int, optional): Maximum length of embeddings (padding applied if needed).\n",
    "            flatten (bool, optional): Whether to flatten the embeddings or not.\n",
    "            embedding_size (int, optional): The size of each embedding.\n",
    "            num_seq (int, optional): The number of sequences (used when `flatten=False`).\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the embeddings and the target label.\n",
    "\n",
    "        Raises:\n",
    "            IndexError: If the index is out of bounds.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data = df\n",
    "        self.embeddings_col_name = embeddings_col_name\n",
    "        self.label_col_name = label_col_name\n",
    "        self.max_length = max_length\n",
    "        self.flatten = flatten\n",
    "        self.num_seq = num_seq\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= self.__len__():\n",
    "            raise IndexError\n",
    "\n",
    "        label = self.data.iloc[idx][self.label_col_name] - 1\n",
    "        embeddings = self.data.iloc[idx][self.embeddings_col_name]\n",
    "\n",
    "        # Pad embeddings to max_length if specified\n",
    "        if self.max_length is not None:\n",
    "            if len(embeddings) < self.max_length:\n",
    "                padding = np.zeros(self.max_length - len(embeddings), dtype=float)\n",
    "                embeddings = np.concatenate((embeddings, padding))\n",
    "\n",
    "            # Reshape embeddings if specified and flatten is False\n",
    "            if not self.flatten and self.num_seq is not None and self.embedding_size is not None:\n",
    "                embeddings = embeddings.reshape(self.num_seq, self.embedding_size)\n",
    "\n",
    "        return {\n",
    "            \"embeddings\": torch.tensor(embeddings, dtype=torch.float32),\n",
    "            \"target\":  torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2f09f",
   "metadata": {
    "id": "5lIKLSOOwx1C",
    "outputId": "2e9fa227-a201-4e8d-eedc-b521fa2a618c",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 128\n",
    "VALID_BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 32\n",
    "NUM_PARALLEL_WORKERS = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10154a66",
   "metadata": {
    "id": "w0F-VsmFr446",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training & Evaluation Functions\n",
    "\n",
    "- `compute_accuracy` calculates the accuracy of model predictions given true labels.\n",
    "- `train_loop_fn` handles one training epoch, updating the model's weights based on computed gradients.\n",
    "- `eval_loop_fn` handles one validation epoch, computing the model's performance on the validation set.\n",
    "- `train_and_evaluate` orchestrates the training process, saving checkpoints if specified. It reports metrics after each epoch. If a final model path is provided, it saves the model at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01059636",
   "metadata": {
    "id": "jJLYAeDA4KlV",
    "outputId": "3e2b88c3-00c5-4fb7-aca5-a25e9f3b3737",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): The model's predictions.\n",
    "        labels (torch.Tensor): The true labels.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy score.\n",
    "\n",
    "    \"\"\"\n",
    "    predicted = torch.argmax(outputs.data, dim=1)\n",
    "\n",
    "    predicted = predicted.detach().cpu().numpy()\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "\n",
    "    acc = accuracy_score(labels, predicted)\n",
    "    return acc\n",
    "\n",
    "def train_loop_fn(data_loader, model, optimizer, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Performs one training epoch.\n",
    "\n",
    "    Args:\n",
    "        data_loader (DataLoader): The DataLoader for training data.\n",
    "        model (nn.Module): The neural network model.\n",
    "        optimizer (torch.optim): The optimizer for updating model weights.\n",
    "        loss_fn: The loss function.\n",
    "        device (torch.device): The device to perform computations.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the training loss and accuracy.\n",
    "\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    acc = []\n",
    "\n",
    "    for batch in tqdm(data_loader):\n",
    "        embeddings = batch['embeddings'].to(device, dtype=torch.float32, non_blocking=True)\n",
    "        labels = batch['target'].to(device, dtype=torch.long, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(embeddings.float())\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()*len(labels)\n",
    "        acc.append(compute_accuracy(outputs, labels))\n",
    "\n",
    "    acc = sum(acc)/len(acc)\n",
    "    return train_loss, acc\n",
    "\n",
    "def eval_loop_fn(data_loader, model, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Performs one evaluation epoch.\n",
    "\n",
    "    Args:\n",
    "        data_loader (DataLoader): The DataLoader for validation data.\n",
    "        model (nn.Module): The neural network model.\n",
    "        loss_fn: The loss function.\n",
    "        device (torch.device): The device to perform computations.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the validation loss and accuracy.\n",
    "\n",
    "    \"\"\"\n",
    "    valid_loss = 0.0\n",
    "    acc = []\n",
    "    model.eval()\n",
    "\n",
    "    for batch in data_loader:\n",
    "        embeddings = batch['embeddings'].to(device, dtype=torch.float32, non_blocking=True)\n",
    "        labels = batch['target'].to(device, dtype=torch.long, non_blocking=True)\n",
    "\n",
    "        outputs = model(embeddings.float())\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        valid_loss += loss.item()*len(labels)\n",
    "\n",
    "        acc.append(compute_accuracy(outputs, labels))\n",
    "\n",
    "    acc = sum(acc)/len(acc)\n",
    "\n",
    "    return valid_loss, acc\n",
    "\n",
    "\n",
    "def train_and_evaluate(\n",
    "    model,\n",
    "    train_data_loader, valid_data_loader,\n",
    "    optimizer, loss_fn,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    checkpoint=False,\n",
    "    path=\"model.pt\",\n",
    "    early_stopping_patience=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        train_data_loader (DataLoader): The DataLoader for training data.\n",
    "        valid_data_loader (DataLoader): The DataLoader for validation data.\n",
    "        optimizer (torch.optim): The optimizer for updating model weights.\n",
    "        loss_fn: The loss function.\n",
    "        device (torch.device): The device to perform computations.\n",
    "        num_epochs (int): The number of epochs.\n",
    "        checkpoint (bool, optional): Whether to save model checkpoints.\n",
    "        path (str, optional): The path to save the model.\n",
    "        early_stopping_patience (int, optional): Number of epochs to wait before early stopping.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The best model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Create directory for saving checkpoint model states\n",
    "    if checkpoint:\n",
    "        dirname = path.split(\".\")[0]\n",
    "        checkpoint_path = os.path.join(dirname)\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            shutil.rmtree(checkpoint_path)\n",
    "        os.makedirs(dirname)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train Step\n",
    "        train_loss, train_acc = train_loop_fn(\n",
    "            train_data_loader, model, optimizer, loss_fn, device\n",
    "        )\n",
    "\n",
    "        # Validation Step\n",
    "        valid_loss, valid_acc = eval_loop_fn(valid_data_loader, model, loss_fn, device)\n",
    "\n",
    "        train_loss /= len(train_data_loader.dataset)\n",
    "        valid_loss /= len(valid_data_loader.dataset)\n",
    "\n",
    "        epoch_log = (\n",
    "            f\"Epoch {epoch+1}/{num_epochs},\"\n",
    "            f\" Train Accuracy={train_acc:.4f}, Validation Accuracy={valid_acc:.4f},\"\n",
    "            f\" Train Loss={train_loss:.4f}, Validation Loss={valid_loss:.4f}\"\n",
    "        )\n",
    "        print(epoch_log)\n",
    "\n",
    "        # Check for improvement in validation loss\n",
    "        if valid_loss < best_loss:\n",
    "            # Save checkpoint if needed\n",
    "            if checkpoint:\n",
    "                cp = os.path.join(checkpoint_path, f\"{dirname}_epoch{epoch}_loss{valid_loss:.4f}.pt\")\n",
    "                torch.save(model.state_dict(), cp)\n",
    "                print(f\"Validation loss improved from {best_loss:.4f}--->{valid_loss:.4f}\")\n",
    "                print(f\"Saved Checkpoint to '{cp}'\")\n",
    "\n",
    "            best_loss = valid_loss\n",
    "            best_model = model\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "\n",
    "            # Early stopping condition\n",
    "            if no_improvement_count >= early_stopping_patience:\n",
    "                print(f\"No improvement for {early_stopping_patience} epochs. Stopping early.\")\n",
    "                break\n",
    "\n",
    "    if checkpoint:\n",
    "        # Save the best model\n",
    "        best_model_path = os.path.join(checkpoint_path, f\"{dirname}-best.pt\")\n",
    "        torch.save(best_model.state_dict(), best_model_path)\n",
    "        print(f\"Saved best model to '{os.path.relpath(best_model_path)}'\")\n",
    "\n",
    "    # Save current model\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def test_model(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Tests the model on the test set.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        data_loader (DataLoader): The DataLoader for test data.\n",
    "        device (torch.device): The device to perform computations.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the test accuracy and loss.\n",
    "\n",
    "    \"\"\"\n",
    "    test_loss = 0.0\n",
    "    acc = []\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval()\n",
    "    for batch in tqdm(data_loader):\n",
    "        embeddings = batch['embeddings'].to(device, dtype=torch.float32)\n",
    "        y_true = batch[\"target\"].to(device, dtype=torch.long)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(embeddings)\n",
    "\n",
    "        loss = loss_fn(y_pred, y_true)\n",
    "        test_loss += loss.item()*len(y_true)\n",
    "\n",
    "        acc.append(compute_accuracy(y_pred, y_true))\n",
    "\n",
    "    acc = sum(acc)/len(acc)\n",
    "    test_loss = test_loss/len(data_loader.dataset)\n",
    "    return acc, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c47b9",
   "metadata": {
    "id": "pkwLebfVPqz5",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ca9c9",
   "metadata": {
    "id": "rsTv5qbAy8w5",
    "outputId": "278a733f-5112-4bcf-b59a-55820aabfc33",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_input_features, num_classes):\n",
    "        \"\"\"\n",
    "        Multi-Layer Perceptron (MLP) for classification tasks.\n",
    "\n",
    "        Args:\n",
    "            num_input_features (int): Number of input features.\n",
    "            num_classes (int): Number of output classes.\n",
    "\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        # Input size is 300 (Word2Vec dimensions)\n",
    "        self.fc1 = nn.Linear(num_input_features, 50)\n",
    "        self.fc2 = nn.Linear(50, 5)\n",
    "        # Output size is 2 for binary classification\n",
    "        self.fc3 = nn.Linear(5, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c7f8e",
   "metadata": {
    "id": "aZFtxV_gbYTi",
    "outputId": "2ea670a3-e5ba-4f40-f3bd-63ae292bdef9",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = AmazonReviewsSentimentDataset(\n",
    "    train_df, embeddings_col_name=\"embeddings\", label_col_name=\"sentiment\"\n",
    ")\n",
    "\n",
    "valid_dataset = AmazonReviewsSentimentDataset(\n",
    "    valid_df, embeddings_col_name=\"embeddings\", label_col_name=\"sentiment\"\n",
    ")\n",
    "\n",
    "test_dataset = AmazonReviewsSentimentDataset(\n",
    "    test_df, embeddings_col_name=\"embeddings\", label_col_name=\"sentiment\"\n",
    ")\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    # num_workers=NUM_PARALLEL_WORKERS\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    # num_workers=NUM_PARALLEL_WORKERS\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    # num_workers=NUM_PARALLEL_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53cebb4",
   "metadata": {
    "id": "Pa9YhzAX3UAO",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## With average Word2Vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b255a",
   "metadata": {
    "id": "wOQq-LKmEae-",
    "outputId": "b8c3e40a-db8e-4975-878b-2db4603b2a3f",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = MLP(num_input_features=Word2VecConfig.MAX_LENGTH, num_classes=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "model = train_and_evaluate(\n",
    "    model=net,\n",
    "    train_data_loader=train_data_loader,\n",
    "    valid_data_loader=valid_data_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=criterion,\n",
    "    device=device,\n",
    "    num_epochs=25,\n",
    "    checkpoint=True,\n",
    "    path=\"mlp_w_avg_w2v_feat_v3.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb38601f",
   "metadata": {
    "id": "-jCkEgheHhev",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Overall Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be052b6d",
   "metadata": {
    "id": "HhBPdtmuHn55",
    "outputId": "3f5f43a9-d2fa-4a80-f6ce-b3a2546ca756",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_saved_model = 'mlp_w_avg_w2v_feat_v3.pt'\n",
    "model = MLP(num_input_features=Word2VecConfig.MAX_LENGTH, num_classes=2)\n",
    "model.load_state_dict(torch.load(path_to_saved_model, map_location=device))\n",
    "\n",
    "acc, loss = test_model(model, test_data_loader, device)\n",
    "print(\"Accuracy (Test Dataset):\", round(acc,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd33bd9",
   "metadata": {
    "id": "O5gfsUal4_tO",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## With top 10 Word2Vec features\n",
    "\n",
    "- Embeddings are padded for maintaining consistent input dimensions across different samples in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71994db1",
   "metadata": {
    "id": "wtXjdlYR5DuL",
    "outputId": "c843f652-e585-4cdf-faaa-787327357f92",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = AmazonReviewsSentimentDataset(\n",
    "    train_df,\n",
    "    embeddings_col_name=\"embeddings_top_10\",\n",
    "    label_col_name=\"sentiment\",\n",
    "    max_length=3000,\n",
    "    flatten=True\n",
    ")\n",
    "\n",
    "valid_dataset = AmazonReviewsSentimentDataset(\n",
    "    valid_df,\n",
    "    embeddings_col_name=\"embeddings_top_10\",\n",
    "    label_col_name=\"sentiment\",\n",
    "    max_length=3000,\n",
    "    flatten=True\n",
    ")\n",
    "\n",
    "test_dataset = AmazonReviewsSentimentDataset(\n",
    "    test_df,\n",
    "    embeddings_col_name=\"embeddings_top_10\",\n",
    "    label_col_name=\"sentiment\",\n",
    "    max_length=3000,\n",
    "    flatten=True\n",
    ")\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e3c92",
   "metadata": {
    "id": "aDBzz6gu5ecB",
    "outputId": "165b1cde-8d9b-4a38-d3ab-1447021ecb60",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "net2 = MLP(num_input_features=3000, num_classes=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net2.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "model2 = train_and_evaluate(\n",
    "    model=net2,\n",
    "    train_data_loader=train_data_loader,\n",
    "    valid_data_loader=valid_data_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=criterion,\n",
    "    device=device,\n",
    "    num_epochs=30,\n",
    "    checkpoint=True,\n",
    "    path=\"mlp_w_top10_w2v_feat_v2.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63795b23",
   "metadata": {
    "id": "4l7disUMc5zc",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Overall Accracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a905b8e",
   "metadata": {
    "id": "qsJvcN-0c-qO",
    "outputId": "cba0095c-a004-41e1-b372-dfbdfb955828",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_saved_model = 'mlp_w_top10_w2v_feat_v2.pt'\n",
    "model = MLP(num_input_features=3000, num_classes=2)\n",
    "model.load_state_dict(torch.load(path_to_saved_model, map_location=device))\n",
    "\n",
    "acc, loss = test_model(model, test_data_loader, device)\n",
    "print(\"Accuracy (Test Dataset):\", round(acc,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a88f2",
   "metadata": {
    "id": "gS7VWiKgId3j",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Comparision with Simple Model\n",
    "The LinearSVC model trained on TF-IDF features was the most effective in this scenario, outperforming both simple models and MLP models trained with Word2Vec embeddings.\n",
    "\n",
    "### Conclusion\n",
    "1. **Feature Importance**:\n",
    "    - The choice of features significantly impacts model performance.\n",
    "    - In this case, TF-IDF features proved to be the most informative for sentiment analysis, as evidenced by the high accuracy achieved by LinearSVC with TF-IDF.\n",
    "\n",
    "2. **Complexity vs. Performance**:\n",
    "    - Simple models like Perceptron and LinearSVC can sometimes outperform more complex models.\n",
    "    - This is evident in the case where LinearSVC with TF-IDF outperformed the MLP models.\n",
    "\n",
    "3. **Embedding Selection**:\n",
    "    - Not all embeddings are equally effective. The choice of Word2Vec embeddings, particularly using the average vectors, yielded competitive results, showcasing the importance of using quality word embeddings.\n",
    "\n",
    "4. **Dimensionality Matters**:\n",
    "    - Using only the top 10 Word2Vec embeddings didn't capture enough information for sentiment analysis.\n",
    "    - It's important to consider the dimensionality of the embeddings and how well they represent the underlying semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d15ac9",
   "metadata": {
    "id": "QKbIuQCOPviZ",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345e26d",
   "metadata": {
    "id": "3OzgDtRHk4C5",
    "outputId": "bc166524-4e3c-4ac9-8fac-a74369951d47",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, hidden_size, num_layers, output_size, model_type=\"rnn\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Recurrent Neural Network (RNN) model for sequence data processing.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Dimension of the input features.\n",
    "            hidden_size (int): Number of units in the hidden layers.\n",
    "            num_layers (int): Number of recurrent layers.\n",
    "            output_size (int): Number of output classes.\n",
    "            model_type (str, optional): Type of RNN ('rnn', 'gru', or 'lstm'). Defaults to 'rnn'.\n",
    "\n",
    "        \"\"\"\n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.model_type = model_type\n",
    "\n",
    "        # Initialize the recurrent layer based on model_type\n",
    "        if model_type == \"gru\":\n",
    "            self.layer = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=0.3)\n",
    "        elif model_type == \"lstm\":\n",
    "            self.layer = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.3)\n",
    "        else:\n",
    "            self.layer = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=0.3)\n",
    "\n",
    "        # dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Fully connected layer for final prediction\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Pass input through the recurrent layer\n",
    "        out, _ = self.layer(x, hidden)\n",
    "\n",
    "        # Stack up the model output\n",
    "        # out = out.contiguous().view(-1, self.hidden_size)\n",
    "\n",
    "        # Use the output from the last time step\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # out = self.dropout(out)\n",
    "\n",
    "        # Apply fully connected layer for final prediction\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.model_type == \"lstm\":\n",
    "            hidden = (\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "            )\n",
    "        else:\n",
    "            hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e86dcb8",
   "metadata": {
    "id": "poWeNiuWgkB2",
    "outputId": "5933c081-4279-4e6b-9916-31ad0e828ae6",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = AmazonReviewsSentimentDataset(\n",
    "    train_df,\n",
    "    embeddings_col_name=\"embeddings_top_10\",\n",
    "    label_col_name=\"sentiment\",\n",
    "    max_length=3000,\n",
    "    flatten=False,\n",
    "    embedding_size=Word2VecConfig.EMBEDDING_SIZE,\n",
    "    num_seq=10\n",
    ")\n",
    "\n",
    "valid_dataset = AmazonReviewsSentimentDataset(\n",
    "    valid_df,\n",
    "    embeddings_col_name=\"embeddings_top_10\",\n",
    "    label_col_name=\"sentiment\",\n",
    "    max_length=3000,\n",
    "    flatten=False,\n",
    "    embedding_size=Word2VecConfig.EMBEDDING_SIZE,\n",
    "    num_seq=10\n",
    ")\n",
    "\n",
    "test_dataset = AmazonReviewsSentimentDataset(\n",
    "    test_df,\n",
    "    embeddings_col_name=\"embeddings_top_10\",\n",
    "    label_col_name=\"sentiment\",\n",
    "    max_length=3000,\n",
    "    flatten=False,\n",
    "    embedding_size=Word2VecConfig.EMBEDDING_SIZE,\n",
    "    num_seq=10\n",
    ")\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505b3098",
   "metadata": {
    "id": "snI8RM9J0_jy",
    "outputId": "75ce1cfa-cfbe-4b57-a40e-a73115b2b8d6",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "hidden_size = 10\n",
    "output_size = 2\n",
    "num_layers = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f77b55",
   "metadata": {
    "id": "oqNMpyFirVzQ",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad260a5",
   "metadata": {
    "id": "w0cXg5JZqmEr",
    "outputId": "a0ed2fad-f5d7-4902-949b-6ca549b0b9b0",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "net3 = RNNModel(input_size, hidden_size, num_layers, output_size, model_type=\"rnn\").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net3.parameters(), lr=0.001)\n",
    "\n",
    "model3 = train_and_evaluate(\n",
    "    model=net3,\n",
    "    train_data_loader=train_data_loader,\n",
    "    valid_data_loader=valid_data_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=criterion,\n",
    "    device=device,\n",
    "    num_epochs=20,\n",
    "    checkpoint=True,\n",
    "    path=\"simple_rnn_w2v_feat_v2.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1704676",
   "metadata": {
    "id": "pMao5SLq8Xg7",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Overall Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16092577",
   "metadata": {
    "id": "K9mA2IrKtpPU",
    "outputId": "92f35e81-1021-45b0-e63e-5c0f5ceca1f2",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_saved_model = 'simple_rnn_w2v_feat_v2.pt'\n",
    "model = RNNModel(input_size, hidden_size, num_layers, output_size, model_type=\"rnn\")\n",
    "model.load_state_dict(torch.load(path_to_saved_model, map_location=device))\n",
    "\n",
    "acc, loss = test_model(model, test_data_loader, device)\n",
    "print(\"Accuracy (Test Dataset):\", round(acc,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eba16a4",
   "metadata": {
    "id": "u8y_As-PtENP",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc7e32b",
   "metadata": {
    "id": "CQstY9PeL31j",
    "outputId": "e548ee0f-0916-444e-c91f-7dc27b3e97d7",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "net4 = RNNModel(input_size, hidden_size, num_layers, output_size, model_type=\"gru\").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net4.parameters(), lr=0.001)\n",
    "\n",
    "model4 = train_and_evaluate(\n",
    "    model=net4,\n",
    "    train_data_loader=train_data_loader,\n",
    "    valid_data_loader=valid_data_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=criterion,\n",
    "    device=device,\n",
    "    num_epochs=20,\n",
    "    checkpoint=True,\n",
    "    path=\"gru_w2v_feat_v2.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210b8462",
   "metadata": {
    "id": "spCaDBeI8Zb6",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Overall Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4071920",
   "metadata": {
    "id": "iEdiXWBI5Co5",
    "outputId": "7e1c9ba8-1759-4ea4-edc2-5172458713a6",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_saved_model = 'gru_w2v_feat_v2.pt'\n",
    "model = RNNModel(input_size, hidden_size, num_layers, output_size, model_type=\"gru\")\n",
    "model.load_state_dict(torch.load(path_to_saved_model, map_location=device))\n",
    "\n",
    "acc, loss = test_model(model, test_data_loader, device)\n",
    "print(\"Accuracy (Test Dataset):\", round(acc,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dc1368",
   "metadata": {
    "id": "ADn_BZaptGTK",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd543e4",
   "metadata": {
    "id": "xeH372gsMBoL",
    "outputId": "bb576441-4de7-4bf4-900f-a85d613dc8a8",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "net5 = RNNModel(input_size, hidden_size, num_layers, output_size, model_type=\"lstm\").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net5.parameters(), lr=0.001)\n",
    "\n",
    "model5 = train_and_evaluate(\n",
    "    model=net5,\n",
    "    train_data_loader=train_data_loader,\n",
    "    valid_data_loader=valid_data_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=criterion,\n",
    "    device=device,\n",
    "    num_epochs=20,\n",
    "    checkpoint=True,\n",
    "    path=\"lstm_w2v_feat_v2.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcf38c5",
   "metadata": {
    "id": "6VwAbSoc8bUB",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Overall Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e01b8",
   "metadata": {
    "id": "6S__Z2Mm7Bqo",
    "outputId": "e60f7f6a-3a7a-4721-9882-8b2e897a506e",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_saved_model = 'lstm_w2v_feat_v2.pt'\n",
    "model = RNNModel(input_size, hidden_size, num_layers, output_size, model_type=\"lstm\")\n",
    "model.load_state_dict(torch.load(path_to_saved_model, map_location=device))\n",
    "\n",
    "acc, loss = test_model(model, test_data_loader, device)\n",
    "print(\"Accuracy (Test Dataset):\", round(acc,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b467ab",
   "metadata": {
    "id": "h5ogsMg1agiP",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Conclusion\n",
    "1. **Feature Representations**:\n",
    "   - TF-IDF outperforms Word2Vec across all models.\n",
    "   - Averaged Word2Vec is better than Concatenated Word2Vec.\n",
    "\n",
    "2. **Model Comparisons**:\n",
    "   - SVM outperforms Perceptron consistently.\n",
    "   - MLP with averaged Word2Vec performs better than RNN, GRU, and LSTM with Word2Vec.\n",
    "\n",
    "3. **Recurrent Models**:\n",
    "   - RNN, GRU, and LSTM show similar performance with Word2Vec embeddings.\n",
    "\n",
    "4. **Overall Performance**:\n",
    "   - Highest accuracy (~87%) is achieved with SVM using TF-IDF.\n",
    "\n",
    "Other:\n",
    "- Averaging Word2Vec embeddings seems a more effective representation\n",
    "- SVM model is better at capturing the non-linear relationships in the data compared to the Perceptron\n",
    "- TF-IDF may capture important information more effectively than Word2Vec embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7378322",
   "metadata": {
    "id": "t-1ZHSbQcAIY",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Report\n",
    "- Results from one of the runs\n",
    "\n",
    "| Model | Accuracy | Features |\n",
    "| ----- | -------- | --- |\n",
    "|SVM(LinearSVC)| 0.8781 | TF-IDF |\n",
    "|SVM(LinearSVC)| 0.8321 | mean word2vec |\n",
    "|MLP| 0.8298 | mean word2vec |\n",
    "|Perceptron | 0.8110 | mean word2vec |\n",
    "|GRU| 0.8063 | top 10 word2vec |\n",
    "|LSTM| 0.8023 | top 10 word2vec |\n",
    "|Perceptron| 0.7998 | TF-IDF |\n",
    "|Simple RNN| 0.7765 | top 10 word2vec |\n",
    "|MLP| 0.7761 | top 10 word2vec |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb146e92",
   "metadata": {
    "id": "VQBWS8AQ6fcA",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 298.970132,
   "end_time": "2023-10-22T08:58:13.715228",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-22T08:53:14.745096",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
