# Multimodal Sentiment Analysis

This repository is dedicated to multimodal sentiment analysis, focusing on determining the emotion conveyed in text, audio, and image data.

## Overview

Multimodal sentiment analysis involves analyzing different types of data (text, audio, and image) to understand the sentiment or emotion conveyed. In this project, we aim to develop models that can accurately detect emotions across multiple modalities.

## Datasets

### Text:
- **Amazon Reviews**: A collection of text reviews from Amazon.
  - [Link to dataset](link_here)

### Image:
- **FER2013**: Facial Expression Recognition 2013 dataset, consisting of facial images labeled with various emotions.
  - [Link to dataset](https://drive.google.com/file/d/1I87qkweAeWb24fCEs9_KlC9pbaJOawfK/view?usp=sharing)

### Audio:
- **RAVDESS**: Ryerson Audio-Visual Database of Emotional Speech and Song, containing speech and song recordings expressing various emotions.
  - [Link to dataset](link_here)
- **CREMA-D**: Crowd-sourced Emotional Multimodal Actors Dataset, comprising speech recordings by actors enacting various emotions.
  - [Link to dataset](link_here)
- **TESS**: Toronto Emotional Speech Set, featuring emotional speech recordings by actors.
  - [Link to dataset](link_here)
- **SAVEE**: Surrey Audio-Visual Expressed Emotion database, consisting of acted speech recordings expressing different emotions.
  - [Link to dataset](link_here)



